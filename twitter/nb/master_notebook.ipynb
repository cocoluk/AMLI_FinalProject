{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max's Jupyter Notebook\n",
    "# Applied Machine Learning Intensive\n",
    "# 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import GetOldTweets3 as got\n",
    "from textblob import TextBlob as blob\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# change default settings\n",
    "\n",
    "np.set_printoptions(threshold = sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: organize fuctions by alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_path = 'twitter/'\n",
    "output_folder = 'master/'\n",
    "encoding_type = 'Latin-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    LAST EDITED -- Max @ 7/29/2019 3:24 PM\n",
    "    REVISIONS NEEDED -- Check to see if the user added a forward slash at the end of the folder name\n",
    "    FINAL VERSION? -- FALSE\n",
    "    BUGS -- \n",
    "    \n",
    "    DESCRIPTION:\n",
    "    Creates list of all avalible CSVs and merges them on date column\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    folder -- A string that represents the filepath of the folder that contains the CSVs\n",
    "    merge_like -- A string that represents a path to a CSV that will act as a master file\n",
    "    input_encoding -- A string that represents the encoding format of the CSVs\n",
    "    \n",
    "    RETURNS:\n",
    "    master_merged -- a CSV with all columns from all merges CSVs\n",
    "    simplified_merged -- a simplified version of the master_merged CSV\n",
    "    \"\"\"\n",
    "def merge_alike_csv(input_working_path):\n",
    "    print('WORKING...')\n",
    "    working_path = input_working_path\n",
    "    temp = None\n",
    "    temp = []\n",
    "    for file in os.listdir(working_path):\n",
    "        if file.endswith('.csv') and file.startswith('twitter'):\n",
    "            temp.append(input_working_path+file)\n",
    "\n",
    "    path_list = input_list_of_paths\n",
    "    encoding_type = input_encoding\n",
    "    pass_value = 0\n",
    "    for item in path_list:\n",
    "        if pass_value == 0:\n",
    "            master = pd.read_csv(item, encoding=str(encoding_type))\n",
    "            pass_value += 1\n",
    "        if pass_value == 1:\n",
    "            temp = pd.read_csv(item, encoding=str(encoding_type))\n",
    "            master.append(temp, ignore_index=True)\n",
    "    master = master.sort_values(by=['date'])\n",
    "    simplified = master[['date', 'id','text']]\n",
    "    simplified.drop_duplicates(subset= 'text', keep='first')\n",
    "    \n",
    "    master.reset_index()\n",
    "    simplified.reset_index()\n",
    "    print('DONE.')\n",
    "    return master, simplified\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    LAST EDITED -- Max @ 7/29/2019 3:40 PM\n",
    "    REVISIONS NEEDED -- prompt user before deleteing\n",
    "    FINAL VERSION? -- FALSE\n",
    "    BUGS -- \n",
    "    \n",
    "    DESCRIPTION:\n",
    "    Takes a current used path and cleanses the entire tree | !THIS WILL DELETE EVERYTHING!\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    path -- A string representing the path to a folder to cleanse\n",
    "    \n",
    "    RETURNS:\n",
    "    None\n",
    "    \"\"\"\n",
    "def cleanse_output_folder():\n",
    "    try:\n",
    "        print('WORKING...')\n",
    "        path = working_path + output_folder\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.mkdir(path)\n",
    "        print('OUTPUT FOLDER CLEANSED.')\n",
    "    except Exception as e:\n",
    "        print('ERROR WHILE CLEANING OUTPUT FOLDER.\\n')\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(input_data_frame, input_):\n",
    "    try:\n",
    "        ()\n",
    "        print('WORKING...')\n",
    "        full_master.to_csv(working_path+'full_master.csv'+output_folder, sep=',')\n",
    "        print('DONE.')\n",
    "    except:\n",
    "        print('AN EXCEPTION OCCURRED! (MAKE SURE ALL FILES CLOSED)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    LAST EDITED -- Max @ 7/29/2019 3:40 PM\n",
    "    REVISIONS NEEDED -- \n",
    "    FINAL VERSION? -- FALSE\n",
    "    BUGS -- \n",
    "    \n",
    "    DESCRIPTION:\n",
    "    Saves a file from a from a \n",
    "    \n",
    "    ARGUMENTS:\n",
    "    variable_name -- A variable that contains a Pandas DataFrame or list\n",
    "    type -- A string representing the type of file to save (\"csv\": for a DataFrame or \"txt\": for a list\n",
    "    RETURNS:\n",
    "    None\n",
    "    \"\"\"\n",
    "def save_file(input_list_of_tweet_ids):\n",
    "        try:\n",
    "        name = \"tweets_\"+(str(dt.datetime.now()).replace(\"-\",\"\").replace(\" \",\"\").replace(\":\",\"\").replace(\".\",\"\")+\".csv\")\n",
    "        save_path = str(working_path + output_folder + name)\n",
    "        print(\"ATTEMPTING TO SAVE TO PATH: \" + \"'\" +save_path + \"'\")\n",
    "        \n",
    "        if os.path.exists(working_path + output_folder):\n",
    "            input_dateframe.to_csv(save_path, sep=',', encoding='utf-8')\n",
    "            print(\"SAVE SUCCESSFUL.\")\n",
    "        else:\n",
    "            print(\"NONEXISTENT PATH! | SAVING TO ROOT...\")\n",
    "            input_dateframe.to_csv(save_path, sep='\\t', encoding='utf-8')\n",
    "            print(\"SAVED AS: basic.csv\")\n",
    "    except:\n",
    "        print(\"ERROR | SAVING TO ROOT...\")\n",
    "        input_dateframe.to_csv(save_path, sep='\\t', encoding='utf-8')\n",
    "        print(\"SAVED AS: basic.csv\")\n",
    "\n",
    "    basic_name = \"pool.txt\"\n",
    "    try:\n",
    "        name = \"pool_\"+(str(dt.datetime.now()).replace(\"-\",\"\").replace(\" \",\"\").replace(\":\",\"\").replace(\".\",\"\")+\".txt\")\n",
    "        save_path = str(working_path + output_folder + name)\n",
    "        print(\"ATTEMPTING TO SAVE TO PATH: \" + \"'\" + save_path + \"'\")\n",
    "        if os.path.exists(working_path + output_folder):\n",
    "            with open(save_path, \"w\") as output:\n",
    "                output.write(str(input_list_of_tweet_ids))\n",
    "            print(\"SAVE SUCCESSFUL.\")\n",
    "        else:\n",
    "            print(\"NONEXISTENT PATH! | SAVING TO ROOT...\")\n",
    "            with open(basic_name, \"w\") as output:\n",
    "                output.write(str(input_list_of_tweet_ids))\n",
    "            print(\"SAVED AS:\", basic_name)\n",
    "    except:\n",
    "        print(\"ERROR | SAVING TO ROOT...\")\n",
    "        with open(basic_name, \"w\") as output:\n",
    "            output.write(str(input_list_of_tweet_ids))\n",
    "        print(\"SAVED AS:\", basic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_id_pool(input_list_of_tweet_ids):\n",
    "    print(\"WORKING...\")\n",
    "    save_id_pool(input_list_of_tweet_ids)\n",
    "    tweet_id_pool = []\n",
    "    print(\"ID POOL CLEANSED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_iteration_setup():\n",
    "    try:   \n",
    "        if date_iteration > 0:\n",
    "            print(date_iteration)\n",
    "    except:\n",
    "        date_iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_set():\n",
    "    return 0,[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_twitter_data(input_the_resulting_twitter_csv):\n",
    "    dirty = input_the_resulting_twitter_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING DATES LIST...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "today = str(dt.date.today())\n",
    "tweet_id_pool = []\n",
    "start = dt.datetime.strptime(\"2010-01-01\", \"%Y-%m-%d\")\n",
    "end = dt.datetime.strptime(today, \"%Y-%m-%d\")\n",
    "search_terms = ['cryptocurrency','cryptocurrencies', 'bitcoin', 'ethereum', 'litecoin']\n",
    "sleep_time = 15\n",
    "print(\"BUILDING DATES LIST...\")\n",
    "tweet_DataFrame = pd.DataFrame(columns = [\"created_at\", \"search_term\", \"tweet_id\", \"username\", \"text\", \"favorite_count\",\"retweet_count\",\"geographic_location\",\"permalink\",\"hashtags\",\"mentions\",\"to\"])\n",
    "dates = []\n",
    "date_generated = [start + dt.timedelta(days=x) for x in range(0, (end-start).days)]\n",
    "for date in date_generated:\n",
    "    dates.append(date)\n",
    "dates_length = len(dates)\n",
    "print(\"DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "path_list = populate_path_list(working_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "full_master, simplified_master = merge_all(path_list,encoding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING...\n",
      "OUTPUT FOLDER CLEANSED.\n"
     ]
    }
   ],
   "source": [
    "cleanse_output_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_iteration, tweet_id_pool = crawler_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_iteration\n",
    "tweet_id_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while date_iteration < dates_length:\n",
    "\n",
    "#     #.setTopTweets(False)\n",
    "#     # create checkpoint every 200 iterations\n",
    "    \n",
    "#     if int(date_iteration) % 200 == 0 and int(date_iteration) != 0:\n",
    "#         print(\"\\n\")\n",
    "#         save_id_pool(tweet_id_pool)\n",
    "#         print(\"\\n\")\n",
    "#         save_dataframe(tweet_DataFrame)\n",
    "#         print(\"\\n\")\n",
    "#         print(\"CHECKPOINT SAVED!\")\n",
    "#         time.sleep(60)        \n",
    "        \n",
    "#     try:\n",
    "#         # limit to 1 query a second!\n",
    "#         time.sleep(1)\n",
    "#         # try to remove this later so we wont have to check for this every time\n",
    "#         if date_iteration == 0:\n",
    "#             date_iteration += 1\n",
    "\n",
    "#         for search_term in search_terms:    \n",
    "#             retrieval_success = False\n",
    "#             while retrieval_success != True:\n",
    "#                 clear_output()\n",
    "#                 try:\n",
    "#                     print(\"DAY ITERATION PROGRESS: \" + str(date_iteration) + \"/\" + str(dates_length) + \" | \" + str((round((date_iteration/dates_length)*100,1))) + \"% COMPLETE\")\n",
    "#                     print(\"DATE RANGE:\",str(dates[date_iteration-1]).split(\" \")[0]+\" → \"+str(dates[date_iteration]).split(\" \")[0])\n",
    "#                     print(\"USING SEARCH TERM: '\" + search_term + \"'\")\n",
    "#                     print(\"\\nATTEMPTING TO RETRIEVE TWEETS...\")\n",
    "#                     tweetCriteria = got.manager.TweetCriteria().setQuerySearch(search_term).setSince(str(dates[date_iteration-1]).split(\" \")[0]).setUntil(str(dates[date_iteration]).split(\" \")[0])\n",
    "#                     tweet_pool = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "#                     retrieval_success = True\n",
    "#                 except:\n",
    "#                     retrieval_success = False\n",
    "#                     print(\"\\nRETRIEVAL FAILURE!\\nSLEEPING FOR\", sleep_time,\"SECONDS...\")\n",
    "#                     save_id_pool(tweet_id_pool)\n",
    "#                     save_dataframe(tweet_DataFrame)\n",
    "#                     except_error = 0\n",
    "#                     time.sleep(sleep_time)\n",
    "\n",
    "#             for tweet in tweet_pool:\n",
    "                \n",
    "#                 created_at = tweet.date\n",
    "#                 search_term = search_term\n",
    "#                 tweet_id_string = str(tweet.id)\n",
    "#                 tweet_id = tweet.id\n",
    "#                 username = tweet.username\n",
    "#                 text = tweet.text\n",
    "#                 favorite_count = tweet.favorites\n",
    "#                 retweet_count = tweet.retweets\n",
    "#                 geographic_location = tweet.geo \n",
    "#                 permalink = tweet.permalink\n",
    "#                 hashtags = tweet.hashtags\n",
    "#                 mentions = tweet.mentions\n",
    "#                 to = tweet.to\n",
    "#                 # analysis = list(TextBlob(text).sentiment)\n",
    "\n",
    "#                 # tweet_id_pool.append(tweet_id_string)\n",
    "#                 row = [created_at, search_term, tweet_id_string, username, text, favorite_count,retweet_count,geographic_location,permalink,hashtags,mentions,to]\n",
    "#                 tweet_DataFrame.loc[len(tweet_DataFrame)] = row\n",
    "                \n",
    "                \n",
    "# #                 if tweet_id_string not in tweet_id_pool:\n",
    "# #                     text = text.lower()\n",
    "# #                     split_text = text.replace(\"#\", '').split()\n",
    "# #                     print(split_text)\n",
    "                    \n",
    "# #                     if search_term in split_text:\n",
    "       \n",
    "                        \n",
    "#     except Exception as e:\n",
    "#         print(\"ERROR:\", e)\n",
    "#         print(\"ATEMPTING TO SAVE DATA...\")\n",
    "#         save_id_pool(tweet_id_pool)\n",
    "#         save_dataframe(tweet_DataFrame)\n",
    "#         print(\"DATA SAVED.\")\n",
    "\n",
    "#     date_iteration += 1\n",
    "    \n",
    "# clear_output()\n",
    "\n",
    "# print(\"CRAWLING COMPLETE!\")\n",
    "# print(\"TOTAL DAYS ITERATED:\",date_iteration)\n",
    "# print(\"FULL DATE RANGE:\", str(dates[0]).split(\" \")[0]+\" → \"+str(dates[-1]).split(\" \")[0])\n",
    "# save_id_pool(tweet_id_pool)\n",
    "# save_dataframe(tweet_DataFrame)\n",
    "# print(\"FILES SAVED!\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
