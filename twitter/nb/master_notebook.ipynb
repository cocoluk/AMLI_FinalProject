{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max's Jupyter Notebook\n",
    "# Applied Machine Learning Intensive\n",
    "# 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules ↓\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import GetOldTweets3 as got\n",
    "from textblob import TextBlob\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# change default settings ↓\n",
    "\n",
    "np.set_printoptions(threshold = sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do graph sentement over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    LAST EDITED -- 7/29/2019\n",
    "    REVISIONS NEEDED -- prompt user before deleteing\n",
    "    FINAL VERSION? -- FALSE\n",
    "    BUGS -- \n",
    "    \n",
    "    DESCRIPTION:\n",
    "    Takes a current used path and cleanses the entire tree | !THIS WILL DELETE EVERYTHING!\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    path -- A string representing the path to a folder to cleanse\n",
    "    \n",
    "    RETURNS:\n",
    "    None\n",
    "    \"\"\"\n",
    "def cleanse_id_pool(input_list_of_tweet_ids):\n",
    "        print(\"WORKING...\")\n",
    "        save_id_pool(input_list_of_tweet_ids)\n",
    "        tweet_id_pool = []\n",
    "        print(\"ID POOL CLEANSED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-cf38db1476ef>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-cf38db1476ef>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    try:\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    LAST EDITED:\n",
    "    7/29/2019\n",
    "    \n",
    "    DESCRIPTION:\n",
    "    Takes a current used path and cleanses the entire tree | !THIS WILL DELETE EVERYTHING!\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    path -- A string representing the path to a folder to cleanse\n",
    "    \n",
    "    RETURNS:\n",
    "    None\n",
    "    \"\"\"\n",
    "def purge_data_directory():\n",
    "    try:\n",
    "        print('WORKING...')\n",
    "        path = working_path + output_folder\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.mkdir(path)\n",
    "        print('OUTPUT FOLDER CLEANSED.')\n",
    "    except Exception as e:\n",
    "        print('ERROR WHILE CLEANING OUTPUT FOLDER.\\n')\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     \"\"\"\n",
    "#     LAST EDITED -- 7/29/2019\n",
    "#     REVISIONS NEEDED -- Check to see if the user added a forward slash at the end of the folder name\n",
    "#     FINAL VERSION? -- FALSE\n",
    "#     BUGS -- \n",
    "    \n",
    "#     DESCRIPTION:\n",
    "#     Creates list of all avalible CSVs and merges them on date column\n",
    "    \n",
    "#     ARGUMENTS:\n",
    "#     folder -- A string that represents the filepath of the folder that contains the CSVs\n",
    "#     merge_like -- A string that represents a path to a CSV that will act as a master file\n",
    "#     input_encoding -- A string that represents the encoding format of the CSVs\n",
    "    \n",
    "#     RETURNS:\n",
    "#     master_merged -- a CSV with all columns from all merges CSVs\n",
    "#     simplified_merged -- a simplified version of the master_merged CSV\n",
    "#     \"\"\"\n",
    "# def merge_alike_csv(input_working_path):\n",
    "#     print('WORKING...')\n",
    "#     working_path = input_working_path\n",
    "#     temp = None\n",
    "#     temp = []\n",
    "#     for file in os.listdir(working_path):\n",
    "#         if file.endswith('.csv') and file.startswith('twitter'):\n",
    "#             temp.append(input_working_path+file)\n",
    "\n",
    "#     path_list = input_list_of_paths\n",
    "#     encoding_type = input_encoding\n",
    "#     pass_value = 0\n",
    "#     for item in path_list:\n",
    "#         if pass_value == 0:\n",
    "#             master = pd.read_csv(item, encoding = str(encoding_type))\n",
    "#             pass_value += 1\n",
    "#         if pass_value == 1:\n",
    "#             temp = pd.read_csv(item, encoding = str(encoding_type))\n",
    "#             master.append(temp, ignore_index = True)\n",
    "#     master = master.sort_values(by=['date'])\n",
    "#     simplified = master[['date', 'id','text']]\n",
    "#     simplified.drop_duplicates(subset = 'text', keep='first')\n",
    "    \n",
    "#     master.reset_index()\n",
    "#     simplified.reset_index()\n",
    "#     print('DONE.')\n",
    "#     return master, simplified\n",
    "#     return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LAST EDITED:\n",
    "2019-07-31\n",
    "DESCRIPTION:\n",
    "Saves a variable to the '/twitter/data' directory \n",
    "ARGUMENTS:\n",
    "variable_name -- A variable that contains a Pandas DataFrame or list\n",
    "type -- A string representing the type of file to save\n",
    "        Use 'auto' or '0' to automatically choose based on variable type\n",
    "        Use 'csv' to for saving a DataFrame to CSV file\n",
    "        Use 'txt' to save a list or string to text file\n",
    "encoding_type -- A string representing the encoding to save the file\n",
    "                 Use 'default' to set the file encoding to utf-8\n",
    "RETURNS:\n",
    "The path to the saved file, time at save point\n",
    "\"\"\"\n",
    "def save_file(variable_name, output_type, encoding_type):\n",
    "        try:\n",
    "            vn = variable_name\n",
    "            ot = str(output_type)\n",
    "            et = str(encoding_type).lower()\n",
    "            all_encodings = ['default','ascii', 'big5', 'big5hkscs', 'cp037', 'cp273', 'cp424', 'cp437', 'cp500', 'cp720', 'cp737', 'cp775', 'cp850', 'cp852', 'cp855', 'cp856', 'cp857', 'cp858', 'cp860', 'cp861', 'cp862', 'cp863', 'cp864', 'cp865', 'cp866', 'cp869', 'cp874', 'cp875', 'cp932', 'cp949', 'cp950', 'cp1006', 'cp1026', 'cp1125', 'cp1140', 'cp1250', 'cp1251', 'cp1252', 'cp1253', 'cp1254', 'cp1255', 'cp1256', 'cp1257', 'cp1258', 'cp65001', 'euc_jp', 'euc_jis_2004', 'euc_jisx0213', 'euc_kr', 'gb2312', 'gbk', 'gb18030', 'hz', 'iso2022_jp', 'iso2022_jp_1', 'iso2022_jp_2', 'iso2022_jp_2004', 'iso2022_jp_3', 'iso2022_jp_ext', 'iso2022_kr', 'latin_1', 'iso8859_2', 'iso8859_3', 'iso8859_4', 'iso8859_5', 'iso8859_6', 'iso8859_7', 'iso8859_8', 'iso8859_9', 'iso8859_10', 'iso8859_11', 'iso8859_13', 'iso8859_14', 'iso8859_15', 'iso8859_16', 'johab', 'koi8_r', 'koi8_t', 'koi8_u', 'kz1048', 'mac_cyrillic', 'mac_greek', 'mac_iceland', 'mac_latin2', 'mac_roman', 'mac_turkish', 'ptcp154', 'shift_jis', 'shift_jis_2004', 'shift_jisx0213', 'utf_32', 'utf_32_be', 'utf_32_le', 'utf_16', 'utf_16_be', 'utf_16_le', 'utf_7', 'utf_8', 'utf_8_sig']\n",
    "            if et in all_encodings:     \n",
    "                if et == \"default\":\n",
    "                    et = \"utf-8\"\n",
    "            else:\n",
    "                print(\"'\" + et + \"'\", \"IS NOT A VALID ENCODING TYPE!\")\n",
    "                return(1)\n",
    "            aot = str(type(vn)).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"<\",\"\").replace(\">\",\"\").replace(\"'\",\"\").replace(\".\",\" \").split(\" \")[-1].lower()\n",
    "            data_path = '../data/'\n",
    "            file_name = str(dt.datetime.now()).replace(\"-\",\"\").replace(\" \",\"\").replace(\":\",\"\").replace(\".\",\"\")\n",
    "            file_extension = [\".txt\",\".csv\"]\n",
    "            if os.path.exists(data_path):\n",
    "                if ot.lower() == \"txt\":\n",
    "                    with open(data_path + file_name + file_extension[0], \"w\") as output:\n",
    "                        output.write(str(vn))\n",
    "                elif ot.lower() == \"csv\":\n",
    "                    if aot != \"dataframe\":\n",
    "                        print(\"CANNOT SAVE OBJECT TYPE \" + aot.upper() + \" TO CSV!\")\n",
    "                        return(1)\n",
    "                    else:\n",
    "                        vn.to_csv(data_path + file_name + file_extension[1], sep='\\t', encoding = et)\n",
    "                elif output_type.lower() == \"checkpoint\":\n",
    "                    if os.path.exists(data_path + \"/checkpoints\"):\n",
    "                        pass\n",
    "                    else: os.mkdir(data_path + \"/checkpoints\")\n",
    "                    vn.to_csv(data_path + \"/checkpoints/\" + file_name + file_extension[1], sep='\\t', encoding = et)\n",
    "                elif ot.lower() == \"auto\" or \"0\":\n",
    "                    if aot == \"list\":\n",
    "                        with open(data_path + file_name + file_extension[0], \"w\") as output:\n",
    "                            output.write(str(vn))\n",
    "                    elif aot == \"dataframe\":\n",
    "                        vn.to_csv(data_path + file_name + file_extension[1], sep='\\t', encoding = et)\n",
    "                else:\n",
    "                    print(\"'\" + variable_name + \"'\", \"IS NOT A SUPPORTED OUTPUT TYPE!\")\n",
    "                    return(1)\n",
    "            else:\n",
    "                print(\"FILE PATH\",\"'\" + data_path + \"'\" ,\"DOES NOT EXIST!\")\n",
    "                return(1)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR:\", e)\n",
    "            return(1)\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LAST EDITED:\n",
    "7/31/2019\n",
    "\n",
    "DESCRIPTION:\n",
    "This fuction will crawl through tweets using the module GetOldTweets3 \n",
    "\n",
    "ARGUMENTS:\n",
    "search_terms -- A list with all search search terms to use when searching for tweets\n",
    "start_date -- A string representing the oldest date to retrieve data from (inclusive)\n",
    "              Use 'oldest' to set start_date to the oldest avalible date\n",
    "              Formatting: '%Y-%m-%d' i.e. '2010-01-01'\n",
    "end_date -- A string representing the most recent date to retrieve data from (inclusive)\n",
    "            Use 'today' to set end_date will be set to the current date\n",
    "            Formatting: '%Y-%m-%d' i.e. '2019-12-01'\n",
    "top_tweets -- A boolean representing the type of tweets to pull (True will return top tweets False will return all constrained by grab_limit)\n",
    "grab_limit -- A interger representing the maximum amount of tweets to retrieve\n",
    "              Use None or '0' to retrieve all tweets\n",
    "check_point -- An interger to save the twitter and id_pool every n iterations            \n",
    "sleep_time -- A interger value representing the time to sleep if GetOldTweets3 module fails to pull tweets (minimum value 1 as declaired on the twitter.com/robots.txt)\n",
    "enable_checkpoints -- A boolean that will enable to diable checkpoints every 250 requests\n",
    "\n",
    "RETURNS: \n",
    "DataFrame with all avalible twitter data from user specified dates and search terms\n",
    "\"\"\" \n",
    "def tweet_crawler(search_terms, since, until, top_tweets, sleep_time, limit, enable_checkpoints):\n",
    "    #try:\n",
    "        st = search_terms\n",
    "        # set since (\"2006-03-21 is the date of the first tweet\")\n",
    "        if since == \"oldest\":\n",
    "            s = dt.datetime.strptime(\"2006-03-21\", \"%Y-%m-%d\")\n",
    "        else: \n",
    "            s = dt.datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        # set until\n",
    "        if until == \"today\":\n",
    "            u = dt.datetime.today()\n",
    "        else: \n",
    "            u = dt.datetime.strptime(u, \"%Y-%m-%d\")\n",
    "        # set top_tweets\n",
    "        tt = bool(top_tweets)\n",
    "        # set sleep_time\n",
    "        if sleep_time < 1:\n",
    "            st = 1\n",
    "        else:\n",
    "            st = sleep_time\n",
    "        # set limit\n",
    "        if limit == 0:\n",
    "            l = 999999999 \n",
    "        else:\n",
    "            l = limit\n",
    "        # set checkpoints\n",
    "        ec = bool(enable_checkpoints)\n",
    "        # create empty tweet pool list, dates list and zero date_iteration value\n",
    "        tweet_id_pool, dates, date_iteration, retrievel_failure_count = [], [], 0, 0\n",
    "        # create DataFrame columns\n",
    "        local_DataFrame = pd.DataFrame(columns = [\"created_at\", \"search_term\", \"tweet_id\", \"username\", \"text\", \"favorite_count\",\"retweet_count\",\"geographic_location\",\"permalink\",\"hashtags\",\"mentions\",\"to\"])\n",
    "        # create dates list\n",
    "        # generate dates based on user input \n",
    "        date_generated = [s + dt.timedelta(days = x) for x in range(0, (u - s).days)]\n",
    "        for date in date_generated:\n",
    "            dates.append(date)\n",
    "        dates_length = len(dates)\n",
    "        while date_iteration < dates_length:\n",
    "            retrieval_failure_count = 0\n",
    "            for search_term in search_terms:  \n",
    "                if enable_checkpoints == True:\n",
    "                    if int(date_iteration) % 250 == 0 and int(date_iteration) != 0:\n",
    "                        save_file(local_DataFrame, \"checkpoint\", \"default\")\n",
    "                    retrieval_success = False\n",
    "                    while retrieval_success != True and retrievel_failure_count <= 8:\n",
    "                        try:\n",
    "                            tweetCriteria = got.manager.TweetCriteria().setQuerySearch(search_term).setSince(str(dates[date_iteration-1]).split(\" \")[0]).setUntil(str(dates[date_iteration]).split(\" \")[0]).setTopTweets(tt)\n",
    "                            tweet_pool = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "                            retrieval_success = True\n",
    "                        except:\n",
    "                            retrieval_failure_count += 1\n",
    "                            time.sleep(15)\n",
    "                            retrieval_success = False\n",
    "                    for tweet in tweet_pool:\n",
    "                            # store all data about tweet\n",
    "                            created_at = tweet.date\n",
    "                            tweet_id_string = tweet.id\n",
    "                            tweet_id = tweet.id\n",
    "                            username = tweet.username\n",
    "                            text = tweet.text\n",
    "                            favorite_count = tweet.favorites\n",
    "                            retweet_count = tweet.retweets\n",
    "                            geographic_location = tweet.geo \n",
    "                            permalink = tweet.permalink\n",
    "                            hashtags = tweet.hashtags\n",
    "                            mentions = tweet.mentions\n",
    "                            to = tweet.to\n",
    "                            analysis = list(TextBlob(text).sentiment)\n",
    "                            # stage a new line to add to localDataFrame\n",
    "                            row = [created_at, search_term, tweet_id_string, username, text, favorite_count,retweet_count,geographic_location,permalink,hashtags,mentions,to]\n",
    "                            local_DataFrame.loc[len(local_DataFrame)] = row\n",
    "                    time.sleep(st)\n",
    "                    clear_output()\n",
    "                    print(\"DAY ITERATION PROGRESS: \" + str(date_iteration) + \"/\" + str(dates_length) + \" | \" + str((round((date_iteration/dates_length)*100,1))) + \"% COMPLETE\")\n",
    "                date_iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = [\"cryptocurrency\", \"fishing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAY ITERATION PROGRESS: 4/149 | 2.7% COMPLETE\n"
     ]
    }
   ],
   "source": [
    "tweet_crawler(search_terms,\"2019-03-07\", \"today\", True, 1, 0, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
