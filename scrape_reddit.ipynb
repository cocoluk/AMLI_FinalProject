{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.test.is_gpu_available()\n",
    "assert tf.test.is_built_with_cuda()\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def extract(json, out_dict):\n",
    "    for item in json['data']:\n",
    "        out_dict['title'].append(item['title'])\n",
    "        out_dict['score'].append(item['score'])\n",
    "        out_dict['id'].append(item['id'])\n",
    "        out_dict['comments'].append(item['num_comments'])\n",
    "        out_dict['created'].append(item['created_utc'])\n",
    "        try:\n",
    "            out_dict['body'].append(item['selftext'])\n",
    "        except KeyError:\n",
    "            out_dict['body'].append('')\n",
    "        try:\n",
    "            out_dict['subreddit_subscribers'].append(item['subreddit_subscribers'])\n",
    "        except KeyError:\n",
    "            out_dict['subreddit_subscribers'].append('')\n",
    "\n",
    "def scrape(out_dict, end, iterations, interval, interval_divisor):\n",
    "    error_log = []\n",
    "    for i in range(iterations):\n",
    "        if not i%1000: print(\"Iteration: \",i)\n",
    "        begin = end - interval\n",
    "\n",
    "        url = 'https://api.pushshift.io/reddit/search/submission/?q=cryptocurrency&size=500'\n",
    "        url += '&before={}'.format(int(end.timestamp()))\n",
    "        url += '&after={}'.format(int(begin.timestamp()))\n",
    "\n",
    "        try:\n",
    "            json = requests.get(url).json()\n",
    "            \n",
    "        except ValueError:\n",
    "            try:\n",
    "                json = requests.get(url).json()\n",
    "                \n",
    "            except ValueError:\n",
    "                error_log.append(url)\n",
    "                \n",
    "\n",
    "        if len(json['data']) == 500:\n",
    "            print(end)\n",
    "            scrape(out_dict, end, interval_divisor, (interval/interval_divisor), interval_divisor)\n",
    "\n",
    "        else:\n",
    "#             for item in json['data']:\n",
    "#                 writer.writerow(list(item.values()))\n",
    "          extract(json, out_dict)\n",
    "\n",
    "        end = begin\n",
    "        \n",
    "    return error_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "2019-06-18 19:47:55.202275\n",
      "Iteration:  0\n",
      "2019-06-18 13:47:55.202275\n",
      "Iteration:  0\n",
      "Iteration:  1000\n",
      "2018-09-25 19:47:55.202275\n",
      "Iteration:  0\n",
      "2018-07-31 13:47:55.202275\n",
      "Iteration:  0\n",
      "2018-07-31 07:47:55.202275\n",
      "Iteration:  0\n",
      "2018-07-31 01:47:55.202275\n",
      "Iteration:  0\n",
      "2018-07-30 19:47:55.202275\n",
      "Iteration:  0\n",
      "2018-07-30 13:47:55.202275\n",
      "Iteration:  0\n",
      "2018-07-29 01:47:55.202275\n",
      "Iteration:  0\n",
      "2018-07-28 19:47:55.202275\n",
      "Iteration:  0\n",
      "2018-07-28 13:47:55.202275\n",
      "Iteration:  0\n",
      "2018-07-27 19:47:55.202275\n",
      "Iteration:  0\n",
      "Iteration:  2000\n",
      "2018-01-31 13:47:55.202275\n",
      "Iteration:  0\n",
      "2018-01-17 19:47:55.202275\n",
      "Iteration:  0\n",
      "2018-01-17 01:47:55.202275\n",
      "Iteration:  0\n",
      "2018-01-16 19:47:55.202275\n",
      "Iteration:  0\n",
      "2018-01-12 19:47:55.202275\n",
      "Iteration:  0\n",
      "2018-01-08 19:47:55.202275\n",
      "Iteration:  0\n",
      "Iteration:  3000\n",
      "Iteration:  4000\n",
      "Iteration:  5000\n",
      "Iteration:  6000\n",
      "Iteration:  7000\n",
      "Iteration:  8000\n",
      "Iteration:  9000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-eff2a24f2a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0merror_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval_divisor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reddit.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "data_dict = {\"title\":[],\n",
    "                \"score\":[],\n",
    "                \"id\":[],\n",
    "                \"comments\": [],\n",
    "                \"created\": [],\n",
    "                \"body\":[],\n",
    "                \"subreddit_subscribers\":[]}\n",
    "\n",
    "end = dt.datetime.today()\n",
    "\n",
    "interval = dt.timedelta(hours=6)\n",
    "\n",
    "interval_divisor = 6\n",
    "\n",
    "iterations = 9120\n",
    "\n",
    "# with open('one_json.csv', 'w') as file:\n",
    "#     writer = csv.writer(file)\n",
    "error_log = scrape(data_dict, end, iterations, interval, interval_divisor)\n",
    "\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "data.to_csv('reddit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments 577158\n",
      "subreddit 756117\n",
      "subreddit_subscribers 577158\n",
      "score 577158\n",
      "created 577158\n",
      "id 577158\n",
      "title 577158\n",
      "body 577158\n"
     ]
    }
   ],
   "source": [
    "for item in data_dict:\n",
    "    print(item, len(data_dict[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {\"title\":[],\n",
    "                \"id\":[],\n",
    "                \"comments\": [],\n",
    "                \"created\": [],\n",
    "                \"body\":[],\n",
    "                \"score\":[],\n",
    "                \"subreddit_subscribers\":[]}\n",
    "\n",
    "for item in data_dict:\n",
    "    if item != \"subreddit\":\n",
    "        new_dict[item] = data_dict[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(new_dict)\n",
    "\n",
    "data.to_csv('reddit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"title\":[],\n",
    "        \"id\":[],\n",
    "        \"comments\": [],\n",
    "        \"created\": [],\n",
    "        \"body\":[],\n",
    "        \"score\":[],\n",
    "        \"subreddit_subscribers\":[]}\n",
    "for item in error_log:\n",
    "    json = requests.get(item).json()\n",
    "    extract(json, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit.csv', 'a') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for i in range(len(d['comments'])):\n",
    "        line = []\n",
    "        for item in d:\n",
    "            line.append(d[item][i])\n",
    "        writer.writerow(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
